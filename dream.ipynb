{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ╬ DreamStreets: AI-Powered Query and Visualization of Street Networks using GPT-OSS-120b, NetworkX, and LangGraph\n",
    "Author: Adam Munawar Rahman, September 2025\n",
    "\n",
    "**OpenAI Open Model Hackathon Submission** - Extending AskStreets with GPT-OSS-120b's Advanced Reasoning\n",
    "\n",
    "Using powerful open-source libraries like OSMnx and NetworkX, we can retrieve geographic features and street network datasets from OpenStreetMap and persist them as graphs in DuckDB. Then, via a LangGraph ReAct agent powered by GPT-OSS-120b, we feed natural language queries to AI-based tools to execute complex lookups, run advanced graph algorithms, and analyze geospatial data. This agentic app enables meaningful insights into the network properties of the desired geographic location, and empowers us to address real-world infrastructure challenges.\n",
    "\n",
    "## What's New with GPT-OSS-120b\n",
    "This implementation showcases OpenAI's most powerful open-source model (120 billion parameters) running completely offline. GPT-OSS-120b brings superior chain-of-thought reasoning capabilities, enabling more complex multi-step analysis and better code generation for graph algorithms. The model runs locally via Ollama, demonstrating true offline capability - crucial for humanitarian field work where internet connectivity is unreliable.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10+\n",
    "- GPT-OSS-120b model loaded in Ollama\n",
    "- DuckDB for spatial data operations\n",
    "- NetworkX for graph algorithms\n",
    "- No internet connection required after initial setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. System Initialization and GPT-OSS Model Setup\n",
    "\n",
    "We begin by initializing the GPT-OSS-120b model and setting up our environment. The model initialization is done once and reused throughout the session for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "DreamStreets: AI-Powered Street Network Analysis using GPT-OSS-120b\n",
    "Author: Adam Munawar Rahman, September 2025\n",
    "\n",
    "Using GPT-OSS-120b's advanced reasoning capabilities, we can analyze street networks\n",
    "and provide insights for urban planning and humanitarian response scenarios.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Import statements and global configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# Database operations\n",
    "import duckdb\n",
    "\n",
    "# Network analysis libraries\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# LangChain and LLM integration\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Rich console output for showcasing model reasoning\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "from rich import box\n",
    "\n",
    "# Jupyter display utilities\n",
    "from IPython.display import display, Markdown as IPMarkdown, HTML\n",
    "\n",
    "# ============================================================================\n",
    "# Configure OSMnx settings\n",
    "# ============================================================================\n",
    "\n",
    "# Enable caching to avoid redundant API calls\n",
    "ox.settings.use_cache = True\n",
    "# Disable console logging for cleaner output\n",
    "ox.settings.log_console = False\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize global components\n",
    "# ============================================================================\n",
    "\n",
    "# Console for formatted output\n",
    "console = Console()\n",
    "\n",
    "# Global state management dictionary\n",
    "# This holds our graph, database connection, and schema information\n",
    "state = {\n",
    "    'graph': None,        # NetworkX graph object\n",
    "    'db': None,          # DuckDB connection\n",
    "    'schema': {},        # Database and graph schema information\n",
    "}\n",
    "\n",
    "# CRITICAL: Single global LLM instance for performance optimization\n",
    "# Initializing GPT-OSS-120b once prevents repeated model loading\n",
    "llm = None\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DreamStreets System Initialization\")\n",
    "print(\"Using GPT-OSS-120b for advanced street network analysis\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gpt_oss():\n",
    "    \"\"\"\n",
    "    Initialize GPT-OSS-120b model for all analysis components.\n",
    "    \n",
    "    This function loads the 120 billion parameter model once and keeps it in memory\n",
    "    for the entire session. The model runs completely offline via Ollama, demonstrating\n",
    "    the \"Best Local Agent\" capability for the hackathon.\n",
    "    \n",
    "    Returns:\n",
    "        ChatOllama: Initialized LLM instance ready for analysis\n",
    "    \"\"\"\n",
    "    global llm\n",
    "    \n",
    "    # Check if already initialized to avoid redundant loading\n",
    "    if llm is not None:\n",
    "        return llm\n",
    "    \n",
    "    console.print(Panel.fit(\n",
    "        \"[bold cyan]▶ Initializing GPT-OSS-120b[/bold cyan]\\n\"\n",
    "        \"[dim]OpenAI's most powerful open-source model with advanced chain-of-thought reasoning[/dim]\\n\"\n",
    "        \"[dim]This one-time setup takes ~30 seconds but ensures optimal performance[/dim]\",\n",
    "        border_style=\"cyan\",\n",
    "        box=box.ROUNDED\n",
    "    ))\n",
    "    \n",
    "    print(\"\\nLoading GPT-OSS-120b model weights...\")\n",
    "    print(\"Model size: 120 billion parameters\")\n",
    "    print(\"Running mode: Completely offline via Ollama\")\n",
    "    \n",
    "    # Create single LLM instance with low temperature for consistent reasoning\n",
    "    llm = ChatOllama(model=\"gpt-oss:120b\", temperature=0.1)\n",
    "    \n",
    "    # Warm up the model with an initial query\n",
    "    # This loads the model into memory and prepares it for analysis\n",
    "    with console.status(\"[bold green]Loading model weights and initializing reasoning engine...\"):\n",
    "        warmup_response = llm.invoke(\"Initialize chain-of-thought reasoning for street network analysis.\")\n",
    "        print(f\"Model initialization complete: {len(warmup_response.content)} tokens generated\")\n",
    "    \n",
    "    console.print(\"[bold green]✓ GPT-OSS-120b ready for advanced analysis![/bold green]\\n\")\n",
    "    print(f\"Model successfully loaded and warmed up\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Initialize the model once at notebook start\n",
    "# This prevents repeated loading during tool execution\n",
    "print(\"\\nInitializing GPT-OSS-120b for the session...\")\n",
    "llm = initialize_gpt_oss()\n",
    "print(\"\\nModel initialization complete. Ready for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# II. Loading Street Network Data and Database Setup\n",
    "# ============================================================================\n",
    "\n",
    "Next, we load the street network graph from GraphML format and set up our DuckDB spatial database. The graph represents intersections as nodes and street segments as edges. We also handle the critical data type conversions needed for NetworkX algorithms to function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_environment(graphml_path: str = 'chinatown.graphml', db_path: str = 'chinatown.duckdb'):\n",
    "    \"\"\"\n",
    "    Initialize the street network graph and spatial database for analysis.\n",
    "    \n",
    "    This function performs several critical operations:\n",
    "    1. Loads the street network graph from GraphML format\n",
    "    2. Converts string attributes to proper numeric types (CRITICAL for NetworkX)\n",
    "    3. Connects to DuckDB with spatial extensions\n",
    "    4. Builds schema information for query generation\n",
    "    \n",
    "    Args:\n",
    "        graphml_path: Path to the GraphML file containing street network\n",
    "        db_path: Path to the DuckDB database file\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if initialization successful\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Loading Street Network Environment\")\n",
    "    print(f\"Graph file: {graphml_path}\")\n",
    "    print(f\"Database file: {db_path}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    console.print(f\"\\n[bold]◉ Loading Street Network: {graphml_path}[/bold]\")\n",
    "    \n",
    "    try:\n",
    "        # ============================================================================\n",
    "        # Load the street network graph from GraphML\n",
    "        # ============================================================================\n",
    "        print(\"\\nStep 1: Loading graph from GraphML file...\")\n",
    "        state['graph'] = nx.read_graphml(graphml_path)\n",
    "        print(f\"Graph loaded successfully\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # CRITICAL FIX: Convert string attributes to numeric types\n",
    "        # GraphML loads numeric values as strings, which breaks NetworkX algorithms\n",
    "        # This is essential for distance calculations and centrality metrics\n",
    "        # ============================================================================\n",
    "        console.print(\"[dim]Converting string attributes to numeric types...[/dim]\")\n",
    "        print(\"\\nStep 2: Converting attribute data types...\")\n",
    "        \n",
    "        # Convert edge attributes\n",
    "        edge_conversions = 0\n",
    "        for u, v, data in state['graph'].edges(data=True):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, str) and key in ['length', 'travel_time']:\n",
    "                    try:\n",
    "                        data[key] = float(value)\n",
    "                        edge_conversions += 1\n",
    "                    except (ValueError, TypeError):\n",
    "                        # Some attributes may not be convertible, skip them\n",
    "                        pass\n",
    "        \n",
    "        # Convert node attributes\n",
    "        node_conversions = 0\n",
    "        for node, data in state['graph'].nodes(data=True):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, str) and key in ['x', 'y', 'street_count']:\n",
    "                    try:\n",
    "                        data[key] = float(value)\n",
    "                        node_conversions += 1\n",
    "                    except (ValueError, TypeError):\n",
    "                        # Some attributes may not be convertible, skip them\n",
    "                        pass\n",
    "        \n",
    "        print(f\"Converted {edge_conversions} edge attributes to numeric\")\n",
    "        print(f\"Converted {node_conversions} node attributes to numeric\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Connect to DuckDB and enable spatial extensions\n",
    "        # ============================================================================\n",
    "        print(\"\\nStep 3: Connecting to DuckDB database...\")\n",
    "        state['db'] = duckdb.connect(db_path, read_only=False)\n",
    "        \n",
    "        # Install and load spatial extension for geographic queries\n",
    "        state['db'].execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "        print(\"DuckDB connected with spatial extensions enabled\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Build schema information for AI query generation\n",
    "        # ============================================================================\n",
    "        print(\"\\nStep 4: Analyzing database schema...\")\n",
    "        \n",
    "        # Get graph statistics\n",
    "        state['schema'] = {\n",
    "            'nodes': state['graph'].number_of_nodes(),\n",
    "            'edges': state['graph'].number_of_edges(),\n",
    "            'tables': {}\n",
    "        }\n",
    "        \n",
    "        # Get table schemas from database\n",
    "        available_tables = []\n",
    "        for table in ['nodes', 'edges', 'pois']:\n",
    "            try:\n",
    "                cols = state['db'].execute(f\"PRAGMA table_info({table})\").fetchdf()\n",
    "                state['schema']['tables'][table] = cols['name'].tolist()\n",
    "                available_tables.append(table)\n",
    "                print(f\"  Found table '{table}' with {len(cols)} columns\")\n",
    "            except:\n",
    "                # Table might not exist in this database\n",
    "                pass\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Print summary statistics\n",
    "        # ============================================================================\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ENVIRONMENT INITIALIZATION COMPLETE\")\n",
    "        print(\"=\"*80)\n",
    "        console.print(f\"[green]✓ Network loaded:[/green] {state['schema']['nodes']} nodes, {state['schema']['edges']} edges\")\n",
    "        console.print(f\"[green]✓ Database ready:[/green] Tables: {list(state['schema']['tables'].keys())}\")\n",
    "        console.print(f\"[green]✓ All attributes converted[/green] for NetworkX compatibility\")\n",
    "        \n",
    "        # Sample some data for verification\n",
    "        sample_node = list(state['graph'].nodes(data=True))[0]\n",
    "        print(f\"\\nSample node data: {sample_node[0][:10]}... with {len(sample_node[1])} attributes\")\n",
    "        \n",
    "        sample_edge = list(state['graph'].edges(data=True))[0]\n",
    "        print(f\"Sample edge data: ({sample_edge[0][:10]}..., {sample_edge[1][:10]}...) with {len(sample_edge[2])} attributes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]✗ Initialization error: {e}[/red]\")\n",
    "        print(f\"\\nERROR: Failed to initialize environment\")\n",
    "        print(f\"Details: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Initialize with default Chinatown NYC files\n",
    "print(\"\\nInitializing Chinatown, NYC street network for urban analysis...\")\n",
    "initialize_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# III. Defining LLM-based Tools for the ReAct Agent\n",
    "# ============================================================================\n",
    "\n",
    "We define two specialized tools that leverage GPT-OSS-120b's capabilities:\n",
    "1. **Network Analyst**: Generates and executes NetworkX code for graph algorithms\n",
    "2. **Database Analyst**: Creates spatial SQL queries for POI analysis\n",
    "\n",
    "Each tool uses GPT-OSS-120b's chain-of-thought reasoning to understand the query and generate appropriate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def network_analyst(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes street network topology using NetworkX algorithms.\n",
    "    Showcases GPT-OSS-120b's ability to generate complex graph analysis code.\n",
    "    \n",
    "    USE THIS TOOL WHEN:\n",
    "    - Computing network metrics (centrality, connectivity, clustering)\n",
    "    - Finding shortest paths between intersections\n",
    "    - Analyzing network structure and topology\n",
    "    - Calculating accessibility metrics\n",
    "    - Identifying critical nodes or edges\n",
    "    \n",
    "    DO NOT USE WHEN:\n",
    "    - Looking up specific places or POIs\n",
    "    - Querying facility information\n",
    "    - Needing exact addresses or names\n",
    "    \"\"\"\n",
    "    global llm  # Use single global instance for performance\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"⬢ NETWORK ANALYST TOOL ACTIVATED\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    console.print(Panel(\n",
    "        f\"[bold]Task:[/bold] {task[:100]}...\\n\"\n",
    "        f\"[dim]Using GPT-OSS-120b to generate NetworkX analysis code[/dim]\",\n",
    "        title=\"⬢ Network Analyst\",\n",
    "        border_style=\"blue\"\n",
    "    ))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Build comprehensive prompt for GPT-OSS-120b\n",
    "    # We provide detailed context about the graph structure and requirements\n",
    "    # ============================================================================\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert Python programmer specializing in NetworkX library for graph analysis.\n",
    "\n",
    "CRITICAL GRAPH INFORMATION:\n",
    "- Graph object 'G' is a MultiDiGraph with {state['schema']['nodes']} nodes and {state['schema']['edges']} edges\n",
    "- G is already loaded - access it directly as 'G'\n",
    "- ALL node IDs are STRINGS like '5340680144' - NEVER use integers\n",
    "- Node attributes: 'y' (latitude), 'x' (longitude), 'street_count' (connections)\n",
    "- Edge attributes: 'length' (meters as float), 'name' (street name), 'highway' (road type)\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Write Python code as ONE CONTINUOUS BLOCK (no blank lines)\n",
    "2. Always use string node IDs: '5340680144' not 5340680144\n",
    "3. Set FINAL_RESULT variable with your findings\n",
    "4. Keep results concise (top 5-10 items, not all {state['schema']['nodes']} nodes)\n",
    "5. Include lat/lon coordinates in results when relevant\n",
    "\n",
    "EXAMPLE PATTERN:\n",
    "# Calculate centrality\n",
    "centrality = nx.degree_centrality(G)\n",
    "top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "FINAL_RESULT = [{{\n",
    "    \"node_id\": str(node_id),\n",
    "    \"centrality\": round(value, 4),\n",
    "    \"lat\": G.nodes[node_id].get('y', 0),\n",
    "    \"lon\": G.nodes[node_id].get('x', 0)\n",
    "}} for node_id, value in top_nodes]\n",
    "\n",
    "Generate ONLY executable Python code. Add comments to explain your reasoning.\"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # Attempt code generation with retry logic for robustness\n",
    "    # ============================================================================\n",
    "    \n",
    "    for attempt in range(2):  # Allow one retry if needed\n",
    "        if attempt > 0:\n",
    "            console.print(f\"[yellow]Retry {attempt}/1 with enhanced guidance...[/yellow]\")\n",
    "            prompt += \"\\n\\nIMPORTANT: Node IDs must be strings! Use '5340680144' format.\"\n",
    "        \n",
    "        try:\n",
    "            # Get GPT-OSS-120b to generate the analysis code\n",
    "            print(\"\\nInvoking GPT-OSS-120b for code generation...\")\n",
    "            console.print(\"\\n[bold cyan]∵ GPT-OSS-120b Chain-of-Thought:[/bold cyan]\")\n",
    "            \n",
    "            response = llm.invoke(prompt)\n",
    "            \n",
    "            # Extract and display the generated code\n",
    "            code = response.content.strip().replace('```python', '').replace('```', '')\n",
    "            \n",
    "            # Display reasoning process (showcases model capability)\n",
    "            console.print(\"[dim]\" + \"─\" * 60 + \"[/dim]\")\n",
    "            print(\"Generated NetworkX analysis code:\")\n",
    "            for line_num, line in enumerate(code.split('\\n')[:20], 1):\n",
    "                if line.strip().startswith('#'):\n",
    "                    # Comments show the model's reasoning\n",
    "                    console.print(f\"[green]{line}[/green]\")\n",
    "                elif 'FINAL_RESULT' in line:\n",
    "                    # Highlight the key output\n",
    "                    console.print(f\"[bold yellow]{line}[/bold yellow]\")\n",
    "                else:\n",
    "                    console.print(f\"[dim]{line}[/dim]\")\n",
    "            \n",
    "            if len(code.split('\\n')) > 20:\n",
    "                console.print(f\"[dim]... ({len(code.split('\\n')) - 20} more lines of code)[/dim]\")\n",
    "            console.print(\"[dim]\" + \"─\" * 60 + \"[/dim]\")\n",
    "            \n",
    "            # ============================================================================\n",
    "            # Prepare code for execution\n",
    "            # ============================================================================\n",
    "            \n",
    "            # Clean code - remove empty lines and imports\n",
    "            lines = [line for line in code.split('\\n') if line.strip()]\n",
    "            code = '\\n'.join(lines)\n",
    "            \n",
    "            # Remove import statements (already available in namespace)\n",
    "            code = '\\n'.join([line for line in code.split('\\n') \n",
    "                            if not line.strip().startswith('import') and not line.strip().startswith('from')])\n",
    "            \n",
    "            # ============================================================================\n",
    "            # Set up execution namespace with all required objects\n",
    "            # CRITICAL: Graph must be in namespace for algorithms to work\n",
    "            # ============================================================================\n",
    "            \n",
    "            exec_namespace = {\n",
    "                'G': state['graph'],  # The graph object\n",
    "                'nx': nx,            # NetworkX library\n",
    "                'json': json,\n",
    "                'math': math,\n",
    "                'np': np,\n",
    "                'pd': pd,\n",
    "                # Built-in functions needed for analysis\n",
    "                'str': str, 'float': float, 'int': int, 'round': round,\n",
    "                'sorted': sorted, 'len': len, 'min': min, 'max': max,\n",
    "                'list': list, 'dict': dict, 'set': set,\n",
    "                'enumerate': enumerate, 'sum': sum,\n",
    "                'FINAL_RESULT': None  # Will be set by the generated code\n",
    "            }\n",
    "            \n",
    "            print(\"\\nExecuting generated NetworkX code...\")\n",
    "            console.print(\"[bold green]→ Executing analysis...[/bold green]\")\n",
    "            \n",
    "            # Execute with single namespace for variable persistence\n",
    "            exec(code, exec_namespace, exec_namespace)\n",
    "            \n",
    "            # Extract the result\n",
    "            result = exec_namespace.get('FINAL_RESULT')\n",
    "            \n",
    "            if result is not None:\n",
    "                console.print(f\"[green]✓ Analysis complete[/green]\")\n",
    "                print(f\"Result type: {type(result)}\")\n",
    "                print(f\"Result preview: {str(result)[:200]}...\")\n",
    "                return f\"Analysis complete: {json.dumps(result, default=str)}\"\n",
    "            else:\n",
    "                raise ValueError(\"FINAL_RESULT was not set by the generated code\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            console.print(f\"[red]✗ Error: {error_msg}[/red]\")\n",
    "            print(f\"\\nExecution error details: {error_msg}\")\n",
    "            \n",
    "            # Provide specific guidance for common errors\n",
    "            if \"is not in the graph\" in error_msg:\n",
    "                console.print(\"[yellow]Hint: Node IDs must be strings. Use '5340680144' not 5340680144[/yellow]\")\n",
    "                print(\"The model may have used integer node IDs instead of strings\")\n",
    "            \n",
    "            if attempt == 0:\n",
    "                print(\"Retrying with additional guidance...\")\n",
    "                continue\n",
    "            else:\n",
    "                return f\"Network analysis failed: {error_msg}. Try rephrasing the query.\"\n",
    "    \n",
    "    return \"Network analysis could not be completed after retries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def database_analyst(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries POIs and performs spatial database operations.\n",
    "    Demonstrates GPT-OSS-120b's SQL generation and spatial reasoning.\n",
    "    \n",
    "    USE THIS TOOL WHEN:\n",
    "    - Finding specific places (shops, hospitals, restaurants, etc.)\n",
    "    - Calculating distances to/from POIs\n",
    "    - Counting facilities by type\n",
    "    - Spatial queries (within distance, nearest neighbor)\n",
    "    - Filtering POIs by attributes\n",
    "    \n",
    "    DO NOT USE WHEN:\n",
    "    - Computing graph algorithms\n",
    "    - Analyzing network topology\n",
    "    - Working only with intersection data\n",
    "    \"\"\"\n",
    "    global llm  # Use single global instance for performance\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"◉ DATABASE ANALYST TOOL ACTIVATED\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    console.print(Panel(\n",
    "        f\"[bold]Task:[/bold] {task[:100]}...\\n\"\n",
    "        f\"[dim]Using GPT-OSS-120b to generate spatial SQL queries[/dim]\",\n",
    "        title=\"◉ Database Analyst\",\n",
    "        border_style=\"green\"\n",
    "    ))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Build comprehensive prompt for SQL generation\n",
    "    # Provide schema details and spatial function examples\n",
    "    # ============================================================================\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert in DuckDB SQL with spatial extensions.\n",
    "\n",
    "DATABASE SCHEMA:\n",
    "\n",
    "Table 'nodes' (street intersections):\n",
    "- node_id: VARCHAR (e.g., '5340680144')\n",
    "- lat: DOUBLE (latitude)\n",
    "- lon: DOUBLE (longitude)  \n",
    "- street_count: INTEGER (number of connecting streets)\n",
    "- geom: GEOMETRY (spatial point geometry)\n",
    "\n",
    "Table 'pois' (points of interest / facilities):\n",
    "- lat: DOUBLE (latitude)\n",
    "- lon: DOUBLE (longitude)\n",
    "- geom: GEOMETRY (spatial point geometry)\n",
    "- amenity: VARCHAR (e.g., 'hospital', 'clinic', 'restaurant', 'school')\n",
    "- building: VARCHAR (building type)\n",
    "- name: VARCHAR (facility name)\n",
    "\n",
    "SPATIAL FUNCTIONS AVAILABLE:\n",
    "- ST_Distance(geom1, geom2): Calculate distance between geometries\n",
    "- ST_Point(lon, lat): Create point geometry\n",
    "- ST_Within(geom, distance): Check if within distance\n",
    "- ST_Buffer(geom, distance): Create buffer around geometry\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Write a single, efficient SQL query\n",
    "2. For medical facilities use: WHERE amenity IN ('hospital', 'clinic', 'health_center')\n",
    "3. Order results appropriately (usually by distance or importance)\n",
    "4. Limit results to reasonable counts (5-20 rows)\n",
    "5. Include comments explaining your approach\n",
    "\n",
    "Provide ONLY the SQL query with comments.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # ============================================================================\n",
    "        # Get GPT-OSS-120b to generate the SQL query\n",
    "        # ============================================================================\n",
    "        \n",
    "        print(\"\\nInvoking GPT-OSS-120b for SQL generation...\")\n",
    "        console.print(\"\\n[bold cyan]∵ GPT-OSS-120b SQL Generation:[/bold cyan]\")\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Extract SQL from response\n",
    "        sql = response.content.strip().replace('```sql', '').replace('```', '')\n",
    "        \n",
    "        # Display SQL with syntax highlighting for readability\n",
    "        console.print(\"[dim]\" + \"─\" * 60 + \"[/dim]\")\n",
    "        print(\"Generated SQL query:\")\n",
    "        for line in sql.split('\\n'):\n",
    "            if line.strip().startswith('--'):\n",
    "                # SQL comments show reasoning\n",
    "                console.print(f\"[green]{line}[/green]\")\n",
    "            elif any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'ORDER', 'LIMIT', 'JOIN', 'GROUP']):\n",
    "                # Highlight SQL keywords\n",
    "                console.print(f\"[bold blue]{line}[/bold blue]\")\n",
    "            else:\n",
    "                console.print(f\"[dim]{line}[/dim]\")\n",
    "        console.print(\"[dim]\" + \"─\" * 60 + \"[/dim]\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Execute the generated SQL query\n",
    "        # ============================================================================\n",
    "        \n",
    "        print(\"\\nExecuting SQL query against DuckDB...\")\n",
    "        console.print(\"[bold green]→ Executing query...[/bold green]\")\n",
    "        \n",
    "        result_df = state['db'].execute(sql).fetchdf()\n",
    "        \n",
    "        # ============================================================================\n",
    "        # Process and format results\n",
    "        # ============================================================================\n",
    "        \n",
    "        console.print(f\"[green]✓ Query returned {len(result_df)} rows[/green]\")\n",
    "        print(f\"\\nQuery execution successful\")\n",
    "        print(f\"Columns returned: {list(result_df.columns)}\")\n",
    "        \n",
    "        if len(result_df) == 0:\n",
    "            print(\"No results found - the requested amenity type may not exist in this area\")\n",
    "            return \"No results found. The requested amenity type may not exist in this area.\"\n",
    "        elif len(result_df) > 20:\n",
    "            print(f\"Large result set - showing top 10 of {len(result_df)} rows\")\n",
    "            return f\"Found {len(result_df)} results. Top 10:\\n{result_df.head(10).to_string()}\"\n",
    "        else:\n",
    "            print(f\"Returning all {len(result_df)} results\")\n",
    "            return f\"Results ({len(result_df)} rows):\\n{result_df.to_string()}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        console.print(f\"[red]✗ Query error: {error_msg}[/red]\")\n",
    "        print(f\"\\nSQL execution error: {error_msg}\")\n",
    "        \n",
    "        # Provide helpful error context\n",
    "        if \"no such table\" in error_msg.lower():\n",
    "            print(\"The requested table does not exist in the database\")\n",
    "        elif \"no such column\" in error_msg.lower():\n",
    "            print(\"The query references a column that doesn't exist\")\n",
    "        \n",
    "        return f\"Database query failed: {error_msg}. Try simplifying the query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(query: str):\n",
    "    \"\"\"\n",
    "    Process urban analysis queries using GPT-OSS-120b's advanced reasoning.\n",
    "    Orchestrates multiple tools to provide comprehensive insights.\n",
    "    \n",
    "    This is the main entry point for all analysis queries. It uses a ReAct agent\n",
    "    to break down complex questions and select appropriate tools.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language question about the street network\n",
    "    \n",
    "    Returns:\n",
    "        str: Analysis results with insights and recommendations\n",
    "    \"\"\"\n",
    "    global llm  # Use single global instance for performance\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Display the query with formatting\n",
    "    # ============================================================================\n",
    "    \n",
    "    console.print(\"\\n\" + \"=\"*70)\n",
    "    console.print(Panel.fit(\n",
    "        f\"[bold cyan]{query}[/bold cyan]\",\n",
    "        title=\"◐ DreamStreets Analysis Query\",\n",
    "        border_style=\"cyan\",\n",
    "        box=box.DOUBLE\n",
    "    ))\n",
    "    console.print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nProcessing query: {query[:100]}...\")\n",
    "    print(f\"Graph info: {state['schema']['nodes']} nodes, {state['schema']['edges']} edges\")\n",
    "    print(f\"Database tables: {list(state['schema']['tables'].keys())}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Set up the tools for the ReAct agent\n",
    "    # ============================================================================\n",
    "    \n",
    "    tools = [network_analyst, database_analyst]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Build enhanced context for the agent\n",
    "    # This helps GPT-OSS-120b understand the available data and tools\n",
    "    # ============================================================================\n",
    "    \n",
    "    enhanced_query = f\"\"\"\n",
    "SYSTEM CONTEXT:\n",
    "- Street network graph 'G' loaded with {state['schema']['nodes']} nodes and {state['schema']['edges']} edges\n",
    "- Database contains: {list(state['schema']['tables'].keys())} tables\n",
    "- All node IDs are STRINGS (e.g., '5340680144')\n",
    "- Numeric attributes (length, x, y) are floats\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "1. network_analyst: Graph algorithms, centrality, paths, network metrics\n",
    "2. database_analyst: Find places, count facilities, spatial queries\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "Provide a comprehensive analysis with specific numbers and actionable insights.\n",
    "Use chain-of-thought reasoning to break down the problem and select appropriate tools.\n",
    "\"\"\"\n",
    "    \n",
    "    console.print(\"[bold]∵ GPT-OSS-120b analyzing query and selecting tools...[/bold]\\n\")\n",
    "    print(\"\\nInitializing ReAct agent with GPT-OSS-120b...\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Create the ReAct agent with our tools\n",
    "    # ============================================================================\n",
    "    \n",
    "    agent = create_react_agent(llm, tools)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"Invoking agent to process query...\")\n",
    "        print(\"This may take a moment as GPT-OSS-120b reasons through the problem...\")\n",
    "        \n",
    "        # Execute the agent with recursion limit for safety\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=enhanced_query)]},\n",
    "            config={\"recursion_limit\": 25}\n",
    "        )\n",
    "        \n",
    "        # Extract the final answer\n",
    "        final_answer = result[\"messages\"][-1].content\n",
    "        \n",
    "    except Exception as e:\n",
    "        if \"recursion limit\" in str(e).lower():\n",
    "            print(\"\\nQuery too complex - reached maximum reasoning depth\")\n",
    "            final_answer = \"⚠ Analysis reached maximum complexity. Try breaking down your query into simpler parts.\"\n",
    "        else:\n",
    "            print(f\"\\nAgent execution error: {str(e)}\")\n",
    "            final_answer = f\"✗ Analysis error: {str(e)}\"\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Display results with formatting\n",
    "    # ============================================================================\n",
    "    \n",
    "    console.print(\"\\n\" + \"=\"*70)\n",
    "    console.print(\"[bold green]◎ ANALYSIS COMPLETE[/bold green]\")\n",
    "    console.print(\"=\"*70)\n",
    "    \n",
    "    # Use Markdown for rich formatting of results\n",
    "    display(IPMarkdown(final_answer))\n",
    "    \n",
    "    # Display performance metrics\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    console.print(f\"[dim]◷ Analysis time: {elapsed:.1f} seconds using GPT-OSS-120b[/dim]\")\n",
    "    console.print(f\"[dim]⚙ Graph: {state['schema']['nodes']} nodes | Database: {len(state['schema']['tables'])} tables[/dim]\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# IV. Urban Planning Analysis - Chinatown, NYC\n",
    "# ============================================================================\n",
    "\n",
    "We begin with dense urban network analysis for business optimization and public safety in NYC's Chinatown. These queries demonstrate how GPT-OSS-120b can help with real-world urban planning decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Optimal Coffee Shop Location\n",
    "\n",
    "Finding the best intersection for a new business based on network centrality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 1: Business location optimization using betweenness centrality\n",
    "# This metric identifies intersections that appear on the most shortest paths\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"I want to open a coffee shop. Which intersection has the highest foot traffic \"\n",
    "    \"based on betweenness centrality? Show me the top 5 locations with coordinates.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Critical Infrastructure Analysis\n",
    "\n",
    "Identifying network bottlenecks that would cause maximum disruption if blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 2: Infrastructure vulnerability assessment\n",
    "# Finding articulation points - nodes whose removal disconnects the network\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"Which intersection is the most critical bottleneck in the network? \"\n",
    "    \"Find nodes whose removal would most increase distances between other nodes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3: Healthcare Accessibility\n",
    "\n",
    "Identifying areas with poor access to medical facilities - critical for public health planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 3: Healthcare desert identification\n",
    "# Finding areas furthest from medical facilities for mobile clinic placement\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"Find the 3 intersections that are furthest from any medical facility. \"\n",
    "    \"These represent healthcare deserts that need mobile clinics.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4: Food Truck Optimization\n",
    "\n",
    "Strategic placement of mobile vendors to maximize coverage while avoiding competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 4: Mobile vendor placement optimization\n",
    "# Using dominating set algorithms to maximize coverage\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"Where should 3 food trucks position themselves to maximize coverage? \"\n",
    "    \"Find intersections that together reach the most nodes within 400m walking distance \"\n",
    "    \"while avoiding existing restaurants.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# V. Humanitarian Response Analysis - Cox's Bazar Refugee Camp\n",
    "# ============================================================================\n",
    "\n",
    "**Context:** The Kutupalong-Balukhali expansion site in Cox's Bazar, Bangladesh is the world's largest refugee settlement with approximately 1 million Rohingya refugees. During monsoon season, flooding isolates communities and disrupts supply chains.\n",
    "\n",
    "**Mission:** Use GPT-OSS-120b's reasoning to optimize emergency resource placement and identify vulnerable areas.\n",
    "\n",
    "This section demonstrates the humanitarian impact potential required for the \"For Humanity\" prize category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Switch to Cox's Bazar refugee camp street network\n",
    "# This network represents the pathways through the camp\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SWITCHING TO HUMANITARIAN CONTEXT\")\n",
    "print(\"Loading Cox's Bazar refugee camp network...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "initialize_environment('coxs_bazar.graphml', 'coxs_bazar.duckdb')\n",
    "\n",
    "print(\"\\nContext: World's largest refugee camp with ~1 million residents\")\n",
    "print(\"Challenge: Monsoon flooding isolates communities\")\n",
    "print(\"Mission: Strategic emergency resource placement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 5: Emergency Evacuation Centers\n",
    "\n",
    "Optimal placement of evacuation centers for rapid access during emergencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 5: Evacuation center placement\n",
    "# Using closeness centrality to find locations with shortest average distances\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"Where should we build emergency evacuation centers? Find the 3 intersections \"\n",
    "    \"with highest closeness centrality that can quickly reach all areas of the camp.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 6: Flood Response Priority\n",
    "\n",
    "Identifying critical points that would isolate communities if flooded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 6: Flood vulnerability assessment\n",
    "# Finding articulation points that would split the network if removed\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"Which intersections are articulation points that would isolate communities if flooded? \"\n",
    "    \"These need elevated platforms for supply distribution during monsoons.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 7: Water Access Vulnerability\n",
    "\n",
    "Critical for cholera prevention - identifying areas at risk of losing water access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 7: Water access vulnerability mapping\n",
    "# Critical for disease prevention in refugee camps\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"Identify areas that would lose water access if main roads flood. \"\n",
    "    \"Find intersections where closure cuts off the most people from water points. \"\n",
    "    \"Critical for cholera prevention.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 8: Medical Emergency Routes\n",
    "\n",
    "Ensuring safe passage for medical emergencies, especially at night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query 8: Emergency medical route optimization\n",
    "# Pregnant women and sick children need safe routes at all hours\n",
    "# ============================================================================\n",
    "\n",
    "result = analyze(\n",
    "    \"Which paths need lighting for night medical emergencies? \"\n",
    "    \"Find shortest routes from the 5 most populated areas to the nearest hospital. \"\n",
    "    \"Pregnant women and sick children can't wait until morning.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# VI. Summary and Impact\n",
    "# ============================================================================\n",
    "\n",
    "## DreamStreets Achievements\n",
    "\n",
    "**DreamStreets** successfully demonstrates GPT-OSS-120b's advanced capabilities for real-world impact:\n",
    "\n",
    "### Technical Innovation\n",
    "- ✓ **Chain-of-thought reasoning** for complex spatial problems\n",
    "- ✓ **Code generation** for NetworkX graph algorithms  \n",
    "- ✓ **SQL synthesis** for spatial database queries\n",
    "- ✓ **Multi-tool orchestration** via ReACT agents\n",
    "- ✓ **Complete offline operation** - no internet required after setup\n",
    "\n",
    "### Real-World Applications\n",
    "- **Urban Planning**: Business location optimization, infrastructure vulnerability\n",
    "- **Public Health**: Healthcare desert identification, emergency access routes\n",
    "- **Humanitarian Response**: Refugee camp management, flood preparedness\n",
    "- **Emergency Management**: Evacuation planning, resource distribution\n",
    "\n",
    "### Hackathon Category Alignment\n",
    "- **Best Local Agent**: Runs completely offline with GPT-OSS-120b via Ollama\n",
    "- **For Humanity**: Direct application to refugee camp management and disaster response\n",
    "- **Best Overall**: Showcases model's reasoning capabilities on critical real-world problems\n",
    "\n",
    "### Impact Metrics\n",
    "- Analyzes networks with thousands of nodes in seconds\n",
    "- Provides actionable insights for decision makers\n",
    "- Scalable to any geographic region with OSM data\n",
    "- No ongoing costs after initial setup (fully local)\n",
    "\n",
    "---\n",
    "\n",
    "_Powered by GPT-OSS-120b - OpenAI's most advanced open-source model_\n",
    "\n",
    "_Building on AskStreets foundation - Winner of ArangoDB GraphRAG Hackathon_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
